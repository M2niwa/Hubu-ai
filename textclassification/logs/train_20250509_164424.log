2025-05-09 16:44:24,762 - INFO - ***** 训练开始 *****
2025-05-09 16:44:24,762 - INFO - 使用设备: cuda | 混合精度启用: True
2025-05-09 16:44:26,364 - INFO - 成功加载预训练模型: models/bert-base-chinese
2025-05-09 16:44:27,272 - INFO - 验证集加载成功，样本数: 10000
2025-05-09 16:44:29,970 - INFO - Epoch 1 | Step 0/6670 | Loss: 3.1658 | LR: 1.00e-08
2025-05-09 16:44:44,833 - INFO - Epoch 1 | Step 100/6670 | Loss: 2.8562 | LR: 1.01e-06
2025-05-09 16:44:59,794 - INFO - Epoch 1 | Step 200/6670 | Loss: 2.7899 | LR: 2.01e-06
2025-05-09 16:45:14,854 - INFO - Epoch 1 | Step 300/6670 | Loss: 2.7197 | LR: 3.01e-06
2025-05-09 16:45:30,225 - INFO - Epoch 1 | Step 400/6670 | Loss: 2.6489 | LR: 4.01e-06
2025-05-09 16:45:46,318 - INFO - Epoch 1 | Step 500/6670 | Loss: 2.5462 | LR: 5.01e-06
2025-05-09 16:46:02,153 - INFO - Epoch 1 | Step 600/6670 | Loss: 2.4203 | LR: 6.01e-06
2025-05-09 16:46:17,484 - INFO - Epoch 1 | Step 700/6670 | Loss: 2.3204 | LR: 7.01e-06
2025-05-09 16:46:33,848 - INFO - Epoch 1 | Step 800/6670 | Loss: 2.2307 | LR: 8.01e-06
2025-05-09 16:46:48,859 - INFO - Epoch 1 | Step 900/6670 | Loss: 2.1578 | LR: 9.01e-06
2025-05-09 16:47:04,120 - INFO - Epoch 1 | Step 1000/6670 | Loss: 2.1023 | LR: 1.00e-05
2025-05-09 16:47:19,128 - INFO - Epoch 1 | Step 1100/6670 | Loss: 2.0431 | LR: 1.10e-05
2025-05-09 16:47:34,146 - INFO - Epoch 1 | Step 1200/6670 | Loss: 2.0020 | LR: 1.20e-05
2025-05-09 16:47:49,207 - INFO - Epoch 1 | Step 1300/6670 | Loss: 1.9622 | LR: 1.30e-05
2025-05-09 16:48:04,251 - INFO - Epoch 1 | Step 1400/6670 | Loss: 1.9285 | LR: 1.40e-05
2025-05-09 16:48:20,031 - INFO - Epoch 1 | Step 1500/6670 | Loss: 1.8910 | LR: 1.50e-05
2025-05-09 16:48:36,480 - INFO - Epoch 1 | Step 1600/6670 | Loss: 1.8639 | LR: 1.60e-05
2025-05-09 16:48:51,571 - INFO - Epoch 1 | Step 1700/6670 | Loss: 1.8406 | LR: 1.70e-05
2025-05-09 16:49:06,463 - INFO - Epoch 1 | Step 1800/6670 | Loss: 1.8173 | LR: 1.80e-05
2025-05-09 16:49:21,375 - INFO - Epoch 1 | Step 1900/6670 | Loss: 1.7973 | LR: 1.90e-05
2025-05-09 16:49:36,076 - INFO - Epoch 1 | Step 2000/6670 | Loss: 1.7809 | LR: 2.00e-05
2025-05-09 16:49:50,955 - INFO - Epoch 1 | Step 2100/6670 | Loss: 1.7641 | LR: 1.99e-05
2025-05-09 16:50:09,553 - INFO - Epoch 1 | Step 2200/6670 | Loss: 1.7448 | LR: 1.98e-05
2025-05-09 16:50:27,908 - INFO - Epoch 1 | Step 2300/6670 | Loss: 1.7311 | LR: 1.97e-05
2025-05-09 16:50:45,900 - INFO - Epoch 1 | Step 2400/6670 | Loss: 1.7186 | LR: 1.96e-05
2025-05-09 16:51:02,598 - INFO - Epoch 1 | Step 2500/6670 | Loss: 1.7049 | LR: 1.94e-05
2025-05-09 16:51:19,482 - INFO - Epoch 1 | Step 2600/6670 | Loss: 1.6980 | LR: 1.93e-05
2025-05-09 16:51:36,099 - INFO - Epoch 1 | Step 2700/6670 | Loss: 1.6889 | LR: 1.92e-05
2025-05-09 16:51:52,325 - INFO - Epoch 1 | Step 2800/6670 | Loss: 1.6791 | LR: 1.91e-05
2025-05-09 16:52:08,576 - INFO - Epoch 1 | Step 2900/6670 | Loss: 1.6675 | LR: 1.90e-05
2025-05-09 16:52:25,306 - INFO - Epoch 1 | Step 3000/6670 | Loss: 1.6598 | LR: 1.89e-05
2025-05-09 16:52:41,720 - INFO - Epoch 1 | Step 3100/6670 | Loss: 1.6486 | LR: 1.88e-05
2025-05-09 16:52:58,568 - INFO - Epoch 1 | Step 3200/6670 | Loss: 1.6378 | LR: 1.87e-05
2025-05-09 16:53:16,621 - INFO - Epoch 1 | Step 3300/6670 | Loss: 1.6298 | LR: 1.86e-05
2025-05-09 16:53:34,196 - INFO - Epoch 1 | Step 3400/6670 | Loss: 1.6196 | LR: 1.84e-05
2025-05-09 16:53:51,537 - INFO - Epoch 1 | Step 3500/6670 | Loss: 1.6126 | LR: 1.83e-05
2025-05-09 16:54:07,930 - INFO - Epoch 1 | Step 3600/6670 | Loss: 1.6067 | LR: 1.82e-05
2025-05-09 16:54:24,295 - INFO - Epoch 1 | Step 3700/6670 | Loss: 1.6009 | LR: 1.81e-05
2025-05-09 16:54:42,426 - INFO - Epoch 1 | Step 3800/6670 | Loss: 1.5937 | LR: 1.80e-05
2025-05-09 16:55:01,612 - INFO - Epoch 1 | Step 3900/6670 | Loss: 1.5860 | LR: 1.79e-05
2025-05-09 16:55:19,958 - INFO - Epoch 1 | Step 4000/6670 | Loss: 1.5807 | LR: 1.78e-05
2025-05-09 16:55:39,011 - INFO - Epoch 1 | Step 4100/6670 | Loss: 1.5746 | LR: 1.77e-05
2025-05-09 16:55:58,987 - INFO - Epoch 1 | Step 4200/6670 | Loss: 1.5693 | LR: 1.76e-05
2025-05-09 16:56:18,516 - INFO - Epoch 1 | Step 4300/6670 | Loss: 1.5639 | LR: 1.74e-05
2025-05-09 16:56:36,499 - INFO - Epoch 1 | Step 4400/6670 | Loss: 1.5593 | LR: 1.73e-05
2025-05-09 16:56:54,950 - INFO - Epoch 1 | Step 4500/6670 | Loss: 1.5555 | LR: 1.72e-05
2025-05-09 16:57:11,702 - INFO - Epoch 1 | Step 4600/6670 | Loss: 1.5510 | LR: 1.71e-05
2025-05-09 16:57:28,176 - INFO - Epoch 1 | Step 4700/6670 | Loss: 1.5450 | LR: 1.70e-05
2025-05-09 16:57:44,750 - INFO - Epoch 1 | Step 4800/6670 | Loss: 1.5384 | LR: 1.69e-05
2025-05-09 16:58:01,362 - INFO - Epoch 1 | Step 4900/6670 | Loss: 1.5326 | LR: 1.68e-05
2025-05-09 16:58:18,652 - INFO - Epoch 1 | Step 5000/6670 | Loss: 1.5271 | LR: 1.67e-05
2025-05-09 16:58:36,199 - INFO - Epoch 1 | Step 5100/6670 | Loss: 1.5237 | LR: 1.66e-05
2025-05-09 16:58:52,155 - INFO - Epoch 1 | Step 5200/6670 | Loss: 1.5199 | LR: 1.64e-05
2025-05-09 16:59:07,881 - INFO - Epoch 1 | Step 5300/6670 | Loss: 1.5158 | LR: 1.63e-05
2025-05-09 16:59:23,598 - INFO - Epoch 1 | Step 5400/6670 | Loss: 1.5127 | LR: 1.62e-05
2025-05-09 16:59:39,756 - INFO - Epoch 1 | Step 5500/6670 | Loss: 1.5075 | LR: 1.61e-05
2025-05-09 16:59:55,808 - INFO - Epoch 1 | Step 5600/6670 | Loss: 1.5038 | LR: 1.60e-05
2025-05-09 17:00:12,713 - INFO - Epoch 1 | Step 5700/6670 | Loss: 1.5003 | LR: 1.59e-05
2025-05-09 17:00:28,610 - INFO - Epoch 1 | Step 5800/6670 | Loss: 1.4969 | LR: 1.58e-05
2025-05-09 17:00:44,736 - INFO - Epoch 1 | Step 5900/6670 | Loss: 1.4936 | LR: 1.57e-05
2025-05-09 17:01:00,908 - INFO - Epoch 1 | Step 6000/6670 | Loss: 1.4909 | LR: 1.56e-05
2025-05-09 17:01:16,923 - INFO - Epoch 1 | Step 6100/6670 | Loss: 1.4886 | LR: 1.54e-05
2025-05-09 17:01:32,941 - INFO - Epoch 1 | Step 6200/6670 | Loss: 1.4850 | LR: 1.53e-05
2025-05-09 17:01:48,984 - INFO - Epoch 1 | Step 6300/6670 | Loss: 1.4823 | LR: 1.52e-05
2025-05-09 17:02:05,657 - INFO - Epoch 1 | Step 6400/6670 | Loss: 1.4791 | LR: 1.51e-05
2025-05-09 17:02:21,654 - INFO - Epoch 1 | Step 6500/6670 | Loss: 1.4764 | LR: 1.50e-05
2025-05-09 17:02:37,601 - INFO - Epoch 1 | Step 6600/6670 | Loss: 1.4731 | LR: 1.49e-05
2025-05-09 17:03:43,005 - INFO - Epoch 1 结果 || 训练损失: 1.4710 | 验证损失: 1.2624 | 准确率: 0.5503 | F1分数: 0.5520
2025-05-09 17:03:43,837 - INFO - 保存最佳模型到: ./checkpoints/best_model_epoch1.bin
2025-05-09 17:03:43,994 - INFO - Epoch 2 | Step 0/6670 | Loss: 0.4907 | LR: 1.48e-05
2025-05-09 17:04:02,179 - INFO - Epoch 2 | Step 100/6670 | Loss: 1.0027 | LR: 1.47e-05
2025-05-09 17:04:19,931 - INFO - Epoch 2 | Step 200/6670 | Loss: 1.0044 | LR: 1.46e-05
2025-05-09 17:04:35,129 - INFO - Epoch 2 | Step 300/6670 | Loss: 1.0324 | LR: 1.45e-05
2025-05-09 17:04:49,480 - INFO - Epoch 2 | Step 400/6670 | Loss: 1.0512 | LR: 1.44e-05
2025-05-09 17:05:02,248 - INFO - Epoch 2 | Step 500/6670 | Loss: 1.0499 | LR: 1.43e-05
2025-05-09 17:05:15,726 - INFO - Epoch 2 | Step 600/6670 | Loss: 1.0530 | LR: 1.41e-05
2025-05-09 17:05:29,895 - INFO - Epoch 2 | Step 700/6670 | Loss: 1.0576 | LR: 1.40e-05
2025-05-09 17:05:43,832 - INFO - Epoch 2 | Step 800/6670 | Loss: 1.0485 | LR: 1.39e-05
2025-05-09 17:05:56,953 - INFO - Epoch 2 | Step 900/6670 | Loss: 1.0468 | LR: 1.38e-05
2025-05-09 17:06:10,561 - INFO - Epoch 2 | Step 1000/6670 | Loss: 1.0444 | LR: 1.37e-05
2025-05-09 17:06:23,719 - INFO - Epoch 2 | Step 1100/6670 | Loss: 1.0497 | LR: 1.36e-05
2025-05-09 17:06:37,390 - INFO - Epoch 2 | Step 1200/6670 | Loss: 1.0513 | LR: 1.35e-05
2025-05-09 17:06:51,176 - INFO - Epoch 2 | Step 1300/6670 | Loss: 1.0539 | LR: 1.34e-05
2025-05-09 17:07:05,286 - INFO - Epoch 2 | Step 1400/6670 | Loss: 1.0553 | LR: 1.33e-05
2025-05-09 17:07:21,818 - INFO - Epoch 2 | Step 1500/6670 | Loss: 1.0575 | LR: 1.31e-05
2025-05-09 17:07:37,790 - INFO - Epoch 2 | Step 1600/6670 | Loss: 1.0595 | LR: 1.30e-05
2025-05-09 17:07:52,910 - INFO - Epoch 2 | Step 1700/6670 | Loss: 1.0592 | LR: 1.29e-05
2025-05-09 17:08:08,293 - INFO - Epoch 2 | Step 1800/6670 | Loss: 1.0607 | LR: 1.28e-05
2025-05-09 17:08:21,966 - INFO - Epoch 2 | Step 1900/6670 | Loss: 1.0625 | LR: 1.27e-05
2025-05-09 17:08:37,636 - INFO - Epoch 2 | Step 2000/6670 | Loss: 1.0638 | LR: 1.26e-05
2025-05-09 17:08:54,336 - INFO - Epoch 2 | Step 2100/6670 | Loss: 1.0665 | LR: 1.25e-05
2025-05-09 17:09:11,144 - INFO - Epoch 2 | Step 2200/6670 | Loss: 1.0671 | LR: 1.24e-05
2025-05-09 17:09:26,627 - INFO - Epoch 2 | Step 2300/6670 | Loss: 1.0644 | LR: 1.23e-05
2025-05-09 17:09:40,741 - INFO - Epoch 2 | Step 2400/6670 | Loss: 1.0625 | LR: 1.21e-05
2025-05-09 17:09:53,916 - INFO - Epoch 2 | Step 2500/6670 | Loss: 1.0630 | LR: 1.20e-05
2025-05-09 17:10:07,554 - INFO - Epoch 2 | Step 2600/6670 | Loss: 1.0615 | LR: 1.19e-05
2025-05-09 17:10:22,079 - INFO - Epoch 2 | Step 2700/6670 | Loss: 1.0586 | LR: 1.18e-05
2025-05-09 17:10:41,034 - INFO - Epoch 2 | Step 2800/6670 | Loss: 1.0603 | LR: 1.17e-05
2025-05-09 17:10:57,019 - INFO - Epoch 2 | Step 2900/6670 | Loss: 1.0607 | LR: 1.16e-05
2025-05-09 17:11:11,432 - INFO - Epoch 2 | Step 3000/6670 | Loss: 1.0613 | LR: 1.15e-05
2025-05-09 17:11:26,199 - INFO - Epoch 2 | Step 3100/6670 | Loss: 1.0596 | LR: 1.14e-05
2025-05-09 17:11:43,064 - INFO - Epoch 2 | Step 3200/6670 | Loss: 1.0589 | LR: 1.13e-05
2025-05-09 17:12:00,415 - INFO - Epoch 2 | Step 3300/6670 | Loss: 1.0591 | LR: 1.11e-05
2025-05-09 17:12:17,573 - INFO - Epoch 2 | Step 3400/6670 | Loss: 1.0586 | LR: 1.10e-05
2025-05-09 17:12:33,759 - INFO - Epoch 2 | Step 3500/6670 | Loss: 1.0598 | LR: 1.09e-05
2025-05-09 17:12:51,068 - INFO - Epoch 2 | Step 3600/6670 | Loss: 1.0579 | LR: 1.08e-05
2025-05-09 17:13:05,673 - INFO - Epoch 2 | Step 3700/6670 | Loss: 1.0574 | LR: 1.07e-05
2025-05-09 17:13:19,120 - INFO - Epoch 2 | Step 3800/6670 | Loss: 1.0566 | LR: 1.06e-05
2025-05-09 17:13:33,258 - INFO - Epoch 2 | Step 3900/6670 | Loss: 1.0553 | LR: 1.05e-05
2025-05-09 17:13:47,085 - INFO - Epoch 2 | Step 4000/6670 | Loss: 1.0565 | LR: 1.04e-05
2025-05-09 17:14:00,645 - INFO - Epoch 2 | Step 4100/6670 | Loss: 1.0556 | LR: 1.03e-05
2025-05-09 17:14:14,462 - INFO - Epoch 2 | Step 4200/6670 | Loss: 1.0557 | LR: 1.01e-05
2025-05-09 17:14:28,658 - INFO - Epoch 2 | Step 4300/6670 | Loss: 1.0559 | LR: 1.00e-05
2025-05-09 17:14:42,403 - INFO - Epoch 2 | Step 4400/6670 | Loss: 1.0555 | LR: 9.93e-06
2025-05-09 17:14:56,233 - INFO - Epoch 2 | Step 4500/6670 | Loss: 1.0543 | LR: 9.82e-06
2025-05-09 17:15:09,715 - INFO - Epoch 2 | Step 4600/6670 | Loss: 1.0546 | LR: 9.71e-06
2025-05-09 17:15:23,535 - INFO - Epoch 2 | Step 4700/6670 | Loss: 1.0539 | LR: 9.59e-06
2025-05-09 17:15:36,690 - INFO - Epoch 2 | Step 4800/6670 | Loss: 1.0547 | LR: 9.48e-06
2025-05-09 17:15:50,668 - INFO - Epoch 2 | Step 4900/6670 | Loss: 1.0522 | LR: 9.37e-06
2025-05-09 17:16:03,682 - INFO - Epoch 2 | Step 5000/6670 | Loss: 1.0512 | LR: 9.26e-06
2025-05-09 17:16:17,126 - INFO - Epoch 2 | Step 5100/6670 | Loss: 1.0528 | LR: 9.15e-06
2025-05-09 17:16:31,419 - INFO - Epoch 2 | Step 5200/6670 | Loss: 1.0540 | LR: 9.04e-06
2025-05-09 17:16:44,928 - INFO - Epoch 2 | Step 5300/6670 | Loss: 1.0549 | LR: 8.93e-06
2025-05-09 17:16:58,646 - INFO - Epoch 2 | Step 5400/6670 | Loss: 1.0541 | LR: 8.82e-06
2025-05-09 17:17:12,467 - INFO - Epoch 2 | Step 5500/6670 | Loss: 1.0550 | LR: 8.71e-06
2025-05-09 17:17:26,267 - INFO - Epoch 2 | Step 5600/6670 | Loss: 1.0544 | LR: 8.59e-06
2025-05-09 17:17:39,083 - INFO - Epoch 2 | Step 5700/6670 | Loss: 1.0550 | LR: 8.48e-06
2025-05-09 17:17:52,199 - INFO - Epoch 2 | Step 5800/6670 | Loss: 1.0553 | LR: 8.37e-06
2025-05-09 17:18:07,571 - INFO - Epoch 2 | Step 5900/6670 | Loss: 1.0546 | LR: 8.26e-06
2025-05-09 17:18:22,851 - INFO - Epoch 2 | Step 6000/6670 | Loss: 1.0553 | LR: 8.15e-06
2025-05-09 17:18:38,253 - INFO - Epoch 2 | Step 6100/6670 | Loss: 1.0549 | LR: 8.04e-06
2025-05-09 17:18:52,911 - INFO - Epoch 2 | Step 6200/6670 | Loss: 1.0537 | LR: 7.93e-06
2025-05-09 17:19:08,326 - INFO - Epoch 2 | Step 6300/6670 | Loss: 1.0519 | LR: 7.82e-06
2025-05-09 17:19:22,994 - INFO - Epoch 2 | Step 6400/6670 | Loss: 1.0517 | LR: 7.71e-06
2025-05-09 17:19:39,018 - INFO - Epoch 2 | Step 6500/6670 | Loss: 1.0513 | LR: 7.60e-06
2025-05-09 17:19:53,942 - INFO - Epoch 2 | Step 6600/6670 | Loss: 1.0513 | LR: 7.48e-06
2025-05-09 17:20:46,865 - INFO - Epoch 2 结果 || 训练损失: 1.0509 | 验证损失: 1.2375 | 准确率: 0.5609 | F1分数: 0.5586
2025-05-09 17:20:47,689 - INFO - 保存最佳模型到: ./checkpoints/best_model_epoch2.bin
2025-05-09 17:20:47,837 - INFO - Epoch 3 | Step 0/6670 | Loss: 0.5420 | LR: 7.41e-06
2025-05-09 17:21:02,095 - INFO - Epoch 3 | Step 100/6670 | Loss: 0.7522 | LR: 7.30e-06
2025-05-09 17:21:15,772 - INFO - Epoch 3 | Step 200/6670 | Loss: 0.7555 | LR: 7.18e-06
2025-05-09 17:21:29,495 - INFO - Epoch 3 | Step 300/6670 | Loss: 0.7522 | LR: 7.07e-06
2025-05-09 17:21:42,753 - INFO - Epoch 3 | Step 400/6670 | Loss: 0.7455 | LR: 6.96e-06
2025-05-09 17:21:55,804 - INFO - Epoch 3 | Step 500/6670 | Loss: 0.7532 | LR: 6.85e-06
2025-05-09 17:22:09,015 - INFO - Epoch 3 | Step 600/6670 | Loss: 0.7375 | LR: 6.74e-06
2025-05-09 17:22:21,982 - INFO - Epoch 3 | Step 700/6670 | Loss: 0.7430 | LR: 6.63e-06
2025-05-09 17:22:34,846 - INFO - Epoch 3 | Step 800/6670 | Loss: 0.7449 | LR: 6.52e-06
2025-05-09 17:22:46,776 - INFO - Epoch 3 | Step 900/6670 | Loss: 0.7428 | LR: 6.41e-06
2025-05-09 17:22:59,648 - INFO - Epoch 3 | Step 1000/6670 | Loss: 0.7320 | LR: 6.30e-06
2025-05-09 17:23:14,822 - INFO - Epoch 3 | Step 1100/6670 | Loss: 0.7410 | LR: 6.18e-06
2025-05-09 17:23:31,490 - INFO - Epoch 3 | Step 1200/6670 | Loss: 0.7465 | LR: 6.07e-06
2025-05-09 17:23:48,124 - INFO - Epoch 3 | Step 1300/6670 | Loss: 0.7473 | LR: 5.96e-06
2025-05-09 17:24:04,557 - INFO - Epoch 3 | Step 1400/6670 | Loss: 0.7488 | LR: 5.85e-06
2025-05-09 17:24:21,125 - INFO - Epoch 3 | Step 1500/6670 | Loss: 0.7487 | LR: 5.74e-06
2025-05-09 17:24:39,281 - INFO - Epoch 3 | Step 1600/6670 | Loss: 0.7509 | LR: 5.63e-06
2025-05-09 17:24:56,084 - INFO - Epoch 3 | Step 1700/6670 | Loss: 0.7512 | LR: 5.52e-06
2025-05-09 17:25:12,862 - INFO - Epoch 3 | Step 1800/6670 | Loss: 0.7502 | LR: 5.41e-06
2025-05-09 17:25:28,314 - INFO - Epoch 3 | Step 1900/6670 | Loss: 0.7501 | LR: 5.30e-06
2025-05-09 17:25:42,196 - INFO - Epoch 3 | Step 2000/6670 | Loss: 0.7509 | LR: 5.19e-06
2025-05-09 17:25:55,714 - INFO - Epoch 3 | Step 2100/6670 | Loss: 0.7523 | LR: 5.07e-06
2025-05-09 17:26:09,276 - INFO - Epoch 3 | Step 2200/6670 | Loss: 0.7536 | LR: 4.96e-06
2025-05-09 17:26:22,741 - INFO - Epoch 3 | Step 2300/6670 | Loss: 0.7522 | LR: 4.85e-06
2025-05-09 17:26:35,231 - INFO - Epoch 3 | Step 2400/6670 | Loss: 0.7532 | LR: 4.74e-06
2025-05-09 17:26:47,861 - INFO - Epoch 3 | Step 2500/6670 | Loss: 0.7547 | LR: 4.63e-06
2025-05-09 17:26:59,944 - INFO - Epoch 3 | Step 2600/6670 | Loss: 0.7536 | LR: 4.52e-06
2025-05-09 17:27:11,948 - INFO - Epoch 3 | Step 2700/6670 | Loss: 0.7536 | LR: 4.41e-06
2025-05-09 17:27:24,138 - INFO - Epoch 3 | Step 2800/6670 | Loss: 0.7562 | LR: 4.30e-06
2025-05-09 17:27:36,637 - INFO - Epoch 3 | Step 2900/6670 | Loss: 0.7572 | LR: 4.19e-06
2025-05-09 17:27:48,816 - INFO - Epoch 3 | Step 3000/6670 | Loss: 0.7570 | LR: 4.07e-06
2025-05-09 17:28:00,842 - INFO - Epoch 3 | Step 3100/6670 | Loss: 0.7562 | LR: 3.96e-06
2025-05-09 17:28:12,860 - INFO - Epoch 3 | Step 3200/6670 | Loss: 0.7585 | LR: 3.85e-06
2025-05-09 17:28:24,770 - INFO - Epoch 3 | Step 3300/6670 | Loss: 0.7588 | LR: 3.74e-06
2025-05-09 17:28:36,641 - INFO - Epoch 3 | Step 3400/6670 | Loss: 0.7601 | LR: 3.63e-06
2025-05-09 17:28:50,396 - INFO - Epoch 3 | Step 3500/6670 | Loss: 0.7593 | LR: 3.52e-06
2025-05-09 17:29:07,539 - INFO - Epoch 3 | Step 3600/6670 | Loss: 0.7583 | LR: 3.41e-06
2025-05-09 17:29:24,643 - INFO - Epoch 3 | Step 3700/6670 | Loss: 0.7584 | LR: 3.30e-06
2025-05-09 17:29:41,591 - INFO - Epoch 3 | Step 3800/6670 | Loss: 0.7579 | LR: 3.19e-06
2025-05-09 17:29:58,506 - INFO - Epoch 3 | Step 3900/6670 | Loss: 0.7585 | LR: 3.08e-06
2025-05-09 17:30:14,296 - INFO - Epoch 3 | Step 4000/6670 | Loss: 0.7599 | LR: 2.96e-06
2025-05-09 17:30:28,481 - INFO - Epoch 3 | Step 4100/6670 | Loss: 0.7602 | LR: 2.85e-06
2025-05-09 17:30:42,844 - INFO - Epoch 3 | Step 4200/6670 | Loss: 0.7603 | LR: 2.74e-06
2025-05-09 17:30:58,264 - INFO - Epoch 3 | Step 4300/6670 | Loss: 0.7599 | LR: 2.63e-06
2025-05-09 17:31:13,174 - INFO - Epoch 3 | Step 4400/6670 | Loss: 0.7596 | LR: 2.52e-06
2025-05-09 17:31:27,251 - INFO - Epoch 3 | Step 4500/6670 | Loss: 0.7588 | LR: 2.41e-06
2025-05-09 17:31:40,263 - INFO - Epoch 3 | Step 4600/6670 | Loss: 0.7591 | LR: 2.30e-06
2025-05-09 17:31:54,590 - INFO - Epoch 3 | Step 4700/6670 | Loss: 0.7582 | LR: 2.19e-06
2025-05-09 17:32:08,388 - INFO - Epoch 3 | Step 4800/6670 | Loss: 0.7580 | LR: 2.08e-06
2025-05-09 17:32:21,521 - INFO - Epoch 3 | Step 4900/6670 | Loss: 0.7581 | LR: 1.96e-06
2025-05-09 17:32:34,462 - INFO - Epoch 3 | Step 5000/6670 | Loss: 0.7582 | LR: 1.85e-06
2025-05-09 17:32:46,396 - INFO - Epoch 3 | Step 5100/6670 | Loss: 0.7596 | LR: 1.74e-06
2025-05-09 17:32:58,082 - INFO - Epoch 3 | Step 5200/6670 | Loss: 0.7600 | LR: 1.63e-06
2025-05-09 17:33:10,958 - INFO - Epoch 3 | Step 5300/6670 | Loss: 0.7592 | LR: 1.52e-06
2025-05-09 17:33:24,019 - INFO - Epoch 3 | Step 5400/6670 | Loss: 0.7597 | LR: 1.41e-06
2025-05-09 17:33:36,729 - INFO - Epoch 3 | Step 5500/6670 | Loss: 0.7577 | LR: 1.30e-06
2025-05-09 17:33:48,774 - INFO - Epoch 3 | Step 5600/6670 | Loss: 0.7572 | LR: 1.19e-06
2025-05-09 17:34:01,101 - INFO - Epoch 3 | Step 5700/6670 | Loss: 0.7573 | LR: 1.08e-06
2025-05-09 17:34:15,957 - INFO - Epoch 3 | Step 5800/6670 | Loss: 0.7576 | LR: 9.65e-07
2025-05-09 17:34:31,911 - INFO - Epoch 3 | Step 5900/6670 | Loss: 0.7578 | LR: 8.54e-07
2025-05-09 17:34:47,220 - INFO - Epoch 3 | Step 6000/6670 | Loss: 0.7578 | LR: 7.43e-07
2025-05-09 17:35:00,725 - INFO - Epoch 3 | Step 6100/6670 | Loss: 0.7578 | LR: 6.32e-07
2025-05-09 17:35:14,400 - INFO - Epoch 3 | Step 6200/6670 | Loss: 0.7577 | LR: 5.21e-07
2025-05-09 17:35:28,463 - INFO - Epoch 3 | Step 6300/6670 | Loss: 0.7581 | LR: 4.10e-07
2025-05-09 17:35:42,324 - INFO - Epoch 3 | Step 6400/6670 | Loss: 0.7571 | LR: 2.99e-07
2025-05-09 17:35:56,112 - INFO - Epoch 3 | Step 6500/6670 | Loss: 0.7563 | LR: 1.88e-07
2025-05-09 17:36:09,872 - INFO - Epoch 3 | Step 6600/6670 | Loss: 0.7555 | LR: 7.66e-08
2025-05-09 17:37:02,198 - INFO - Epoch 3 结果 || 训练损失: 0.7558 | 验证损失: 1.3229 | 准确率: 0.5650 | F1分数: 0.5653
2025-05-09 17:37:03,018 - INFO - 保存最佳模型到: ./checkpoints/best_model_epoch3.bin
